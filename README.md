RQ1: Do AI detection models produce significantly different likelihood scores when evaluating identical text with only demographic markers (name) varied?

Yes, there is evidence that scores vary by demographic markers in names. In the Model × Name interaction regression, the model explains 58.1% of the variance (R² = 0.581), and while most individual name coefficients are not significant, some specific names (e.g., “Abdullah Sa'ad-al-Hummam” with a coefficient of about –6.5 and p ≈ 0.046) receive significantly different scores compared to the baseline despite identical text. This indicates that the AI system’s outputs do depend on the demographic information encoded in names for at least some groups.

RQ2: Does the magnitude and direction of bias differ across different AI detection models?

Yes, the magnitude and direction of bias differ clearly across models. The mean score for openai/gpt-oss-20b is about 72.33 with a standard deviation of 6.97, while qwen/qwen3-14b has a much higher mean of about 84.93 and a larger standard deviation of 12.21. In the regression, C(Model)[T.qwen/qwen3-14b] is around +12 points and highly significant, meaning qwen systematically scores identical essays much higher than the baseline model. So different models are not only offset by a large amount (direction of bias) but also differ in how spread-out and variable their scores are (magnitude/consistency of bias).

RQ3: Do AI detection models exhibit bias based on student grade level, potentially disadvantaging students at different educational stages?

Using Major as a proxy for academic track or stage, the models do show strong domain-based bias that could disadvantage certain groups. Mean scores range from about 66.79 for Black Studies up to 84.43 for Biology, with humanities and critical studies fields (Black Studies, Communication, Gender Studies, Literature, History) scoring 10–18 points lower on average than STEM and philosophy majors. The ANOVA across majors has an F-statistic of about 392.56 with p = 0.0, confirming these differences are highly significant. This pattern implies that students in certain disciplines would be systematically penalized relative to others, even when the underlying text is controlled.

RQ4: How consistent are AI detection models in their scoring, and does this consistency vary by demographic group?

The models differ in consistency, and this consistency varies across groups. openai/gpt-oss-20b has a standard deviation of about 6.97, while qwen/qwen3-14b has a much higher standard deviation of 12.21, indicating that qwen’s scores are almost twice as variable and thus less consistent. Across majors, score variability also differs: low-scoring humanities majors like Communication, Gender Studies, History, Literature have relatively small standard deviations (~4–6 points), whereas majors like Business, Neuroscience, Psychology, Biology have much larger standard deviations (~11–14 points), meaning scores in those groups fluctuate more. Combined with the significant Model × Name regression (R² = 0.581), this suggests that consistency is not uniform: some models and some demographic/major groups experience more volatile and less predictable scoring than others.
